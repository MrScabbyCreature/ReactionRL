{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1cfa57a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run supervised_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b02fba",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536ae7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_mols = pickle.load(open(\"datasets/my_uspto/unique_start_mols.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322bdda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100272it [04:03, 411.06it/s]                                                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100272, 10)\n",
      "(100272, 10)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "N = 100000\n",
    "steps = 1\n",
    "\n",
    "df_list = []\n",
    "final_shape = 0\n",
    "smiles_per_random_sample = 1000\n",
    "pool_chunk_size = 10\n",
    "\n",
    "# Create dataset for multi-step pred\n",
    "with Pool(30) as p, tqdm.tqdm(total=N) as pbar:\n",
    "    while final_shape < N:\n",
    "        smiles = np.random.choice(start_mols, size=(smiles_per_random_sample,))\n",
    "\n",
    "        for new_df in p.imap_unordered(functools.partial(generate_train_data, steps=steps), smiles, chunksize=10):\n",
    "            df_list.append(new_df)\n",
    "            final_shape += new_df.shape[0]\n",
    "            \n",
    "        pbar.update(final_shape - pbar.n)\n",
    "\n",
    "main_df = pd.concat(df_list)\n",
    "del df_list\n",
    "print(main_df.shape)\n",
    "\n",
    "# randomize\n",
    "main_df = pd.concat([main_df[:int(main_df.shape[0]*0.8)].sample(frac=1), main_df[int(main_df.shape[0]*0.8):].sample(frac=1)])\n",
    "print(main_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d58b5a",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3432cfc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb759a",
   "metadata": {},
   "source": [
    "# Helper stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf8b1d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89384, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhor/miniconda3/envs/de_novo/lib/python3.7/site-packages/torchdrug-0.2.0-py3.7.egg/torchdrug/data/feature.py:42: UserWarning: Unknown value `Li`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/home/abhor/miniconda3/envs/de_novo/lib/python3.7/site-packages/torchdrug-0.2.0-py3.7.egg/torchdrug/data/feature.py:42: UserWarning: Unknown value `Ge`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/home/abhor/miniconda3/envs/de_novo/lib/python3.7/site-packages/torchdrug-0.2.0-py3.7.egg/torchdrug/data/feature.py:42: UserWarning: Unknown value `K`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/home/abhor/miniconda3/envs/de_novo/lib/python3.7/site-packages/torchdrug-0.2.0-py3.7.egg/torchdrug/data/feature.py:42: UserWarning: Unknown value `Na`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/home/abhor/miniconda3/envs/de_novo/lib/python3.7/site-packages/torchdrug-0.2.0-py3.7.egg/torchdrug/data/feature.py:42: UserWarning: Unknown value `Ti`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/home/abhor/miniconda3/envs/de_novo/lib/python3.7/site-packages/torchdrug-0.2.0-py3.7.egg/torchdrug/data/feature.py:42: UserWarning: Unknown value `Pb`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/home/abhor/miniconda3/envs/de_novo/lib/python3.7/site-packages/torchdrug-0.2.0-py3.7.egg/torchdrug/data/feature.py:42: UserWarning: Unknown value `Al`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/home/abhor/miniconda3/envs/de_novo/lib/python3.7/site-packages/torchdrug-0.2.0-py3.7.egg/torchdrug/data/feature.py:42: UserWarning: Unknown value `Ga`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/home/abhor/miniconda3/envs/de_novo/lib/python3.7/site-packages/torchdrug-0.2.0-py3.7.egg/torchdrug/data/feature.py:42: UserWarning: Unknown value `Ru`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/home/abhor/miniconda3/envs/de_novo/lib/python3.7/site-packages/torchdrug-0.2.0-py3.7.egg/torchdrug/data/feature.py:42: UserWarning: Unknown value `Ta`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/home/abhor/miniconda3/envs/de_novo/lib/python3.7/site-packages/torchdrug-0.2.0-py3.7.egg/torchdrug/data/feature.py:42: UserWarning: Unknown value `As`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/home/abhor/miniconda3/envs/de_novo/lib/python3.7/site-packages/torchdrug-0.2.0-py3.7.egg/torchdrug/data/feature.py:42: UserWarning: Unknown value `Cr`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/home/abhor/miniconda3/envs/de_novo/lib/python3.7/site-packages/torchdrug-0.2.0-py3.7.egg/torchdrug/data/feature.py:42: UserWarning: Unknown value `Cd`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/home/abhor/miniconda3/envs/de_novo/lib/python3.7/site-packages/torchdrug-0.2.0-py3.7.egg/torchdrug/data/feature.py:42: UserWarning: Unknown value `Pt`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 56s, sys: 17 s, total: 5min 13s\n",
      "Wall time: 4min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "action_dataset = pd.read_csv(\"datasets/my_uspto/action_dataset-filtered.csv\", index_col=0)\n",
    "action_dataset = action_dataset.loc[action_dataset[\"action_tested\"] & action_dataset[\"action_works\"]]\n",
    "action_dataset = action_dataset[[\"rsub\", \"rcen\", \"rsig\", \"rbond\", \"psub\", \"pcen\", \"psig\", \"pbond\"]]\n",
    "print(action_dataset.shape)\n",
    "\n",
    "action_rsigs = data.Molecule.pack(list(map(molecule_from_smile, action_dataset[\"rsig\"])))\n",
    "action_psigs = data.Molecule.pack(list(map(molecule_from_smile, action_dataset[\"psig\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc17df",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c50b019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_lr = 3e-4\n",
    "epochs = 50\n",
    "batch_size = 512\n",
    "\n",
    "#####################\n",
    "# Get actor network #\n",
    "#####################\n",
    "actor = PolicyNetwork().to(device)#\n",
    "actor.eval() # Non-trainable for now\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad309738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 44/44 [00:09<00:00,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([89384, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|█                                                        | 1951/100272 [00:05<02:23, 685.99it/s]"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# Get inital embeddings and applicable_indices+correct_indices #\n",
    "################################################################\n",
    "action_embeddings = get_action_dataset_embeddings(actor.GIN)\n",
    "print(action_embeddings.shape)\n",
    "\n",
    "# I'm storing as lists, so doing numpy operations for the elements\n",
    "correct_applicable_indices = []\n",
    "correct_action_dataset_indices = []\n",
    "action_embedding_indices = []\n",
    "\n",
    "# for indices_used_for_data, correct_idx in tqdm.tqdm(map(get_emb_indices_and_correct_idx, main_df.iterrows()), total=main_df.shape[0]):\n",
    "with Pool(20) as p:\n",
    "    for indices_used_for_data, correct_app_idx, correct_act_idx in tqdm.tqdm(p.imap(get_emb_indices_and_correct_idx, main_df.iterrows(), chunksize=50), total=main_df.shape[0]):\n",
    "        action_embedding_indices.append(indices_used_for_data)\n",
    "        correct_applicable_indices.append(correct_app_idx)\n",
    "        correct_action_dataset_indices.append(correct_act_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training data\n",
    "def make_data(args):\n",
    "    i, negative_topk = args\n",
    "    data = []\n",
    "    targets = []\n",
    "    act_emb_for_i, correct_index = action_embeddings[action_embedding_indices[i]], correct_indices[i]\n",
    "\n",
    "    # Positive\n",
    "    positive = act_emb_for_i[correct_index]\n",
    "\n",
    "    # Calc negatives\n",
    "    dist = np.linalg.norm(action_embeddings - positive, axis=1)\n",
    "    sorted_idx = np.argsort(dist)[:negative_topk] \n",
    "    sorted_idx = sorted_idx[sorted_idx != correct_index] # Remove correct index in list\n",
    "    \n",
    "    data = np.array([(i, correct_index)]+list(zip([i] * len(sorted_idx), sorted_idx)))\n",
    "    targets = [[1] + [0] * len(sorted_idx)]\n",
    "    return data, targets\n",
    "\n",
    "train_X = []\n",
    "train_Y = []\n",
    "test_X = []\n",
    "test_Y = []\n",
    "with Pool(20) as p:\n",
    "    bs = 100\n",
    "    # Train\n",
    "    start, end = 0, int(main_df.shape[0]*0.8)\n",
    "    for X, Y in tqdm.tqdm(\n",
    "                          p.imap_unordered(make_data, \n",
    "                                 [(i, 10) for i in range(start, end)], \n",
    "                                 chunksize=bs), \n",
    "                          total=(end-start)):\n",
    "        train_X.append(X)\n",
    "        train_Y.append(Y)\n",
    "    \n",
    "    # Test\n",
    "    start, end = int(main_df.shape[0]*0.8), main_df.shape[0]\n",
    "    for X, Y in tqdm.tqdm(\n",
    "                          p.imap_unordered(make_data, \n",
    "                                 [(i, 1) for i in range(start, end)], \n",
    "                                 chunksize=bs), \n",
    "                          total=end-start):\n",
    "        test_X.append(X)\n",
    "        test_Y.append(Y)\n",
    "\n",
    "\n",
    "train_X = np.concatenate(train_X, axis=0)\n",
    "test_X = np.concatenate(test_X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3bd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_reactants = data.Molecule.pack(list(map(molecule_from_smile, main_df.iloc[train_X[:, 0]][\"reactant\"])))\n",
    "%time train_products = data.Molecule.pack(list(map(molecule_from_smile, main_df.iloc[train_X[:, 0]][\"product\"])))\n",
    "%time train_rsigs = data.Molecule.pack(list(map(molecule_from_smile, action_dataset.iloc[train_X[:, 1]][\"rsig\"])))\n",
    "%time train_psigs = data.Molecule.pack(list(map(molecule_from_smile, action_dataset.iloc[train_X[:, 1]][\"psig\"])))\n",
    "\n",
    "test_reactants = data.Molecule.pack(list(map(molecule_from_smile, main_df.iloc[test_X[:, 0]][\"reactant\"])))\n",
    "test_products = data.Molecule.pack(list(map(molecule_from_smile, main_df.iloc[test_X[:, 0]][\"product\"])))\n",
    "test_rsigs = data.Molecule.pack(list(map(molecule_from_smile, action_dataset.iloc[test_X[:, 1]][\"rsig\"])))\n",
    "test_psigs = data.Molecule.pack(list(map(molecule_from_smile, action_dataset.iloc[test_X[:, 1]][\"psig\"])))\n",
    "\n",
    "train_Y = torch.Tensor(np.concatenate(train_Y, axis=1)).view(-1, 1)\n",
    "test_Y = torch.Tensor(np.concatenate(test_Y, axis=1)).view(-1, 1)\n",
    "\n",
    "print(train_reactants.batch_size, train_products.batch_size, train_rsigs.batch_size, train_psigs.batch_size, train_Y.shape)\n",
    "\n",
    "print(test_reactants.batch_size, test_products.batch_size, test_rsigs.batch_size, test_psigs.batch_size, test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cbce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3da10d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# Critic #\n",
    "##########\n",
    "critic_lr = 1e-3\n",
    "epochs = 5\n",
    "num_hidden = 2\n",
    "hidden_size = 256\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()    \n",
    "critic = CriticNetwork().to(device)\n",
    "print(critic.DENSE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(critic.parameters(), lr=critic_lr)  \n",
    "\n",
    "loss_list = []\n",
    "test_loss = []\n",
    "\n",
    "# Train the critic\n",
    "num_batch_per_gpu_transfer = 100\n",
    "for epoch in range(epochs):\n",
    "    critic.train()\n",
    "    for i in tqdm.tqdm(range(0, train_reactants.batch_size - batch_size, batch_size)):\n",
    "        if i % (batch_size * num_batch_per_gpu_transfer) == 0:\n",
    "            batch_reactants = train_reactants[i:min(i+batch_size*num_batch_per_gpu_transfer, train_reactants.batch_size)].to(device)\n",
    "            batch_products = train_products[i:min(i+batch_size*num_batch_per_gpu_transfer, train_reactants.batch_size)].to(device)\n",
    "            batch_rsigs = train_rsigs[i:min(i+batch_size*num_batch_per_gpu_transfer, train_reactants.batch_size)].to(device)\n",
    "            batch_psigs = train_psigs[i:min(i+batch_size*num_batch_per_gpu_transfer, train_reactants.batch_size)].to(device)\n",
    "            batch_Y = train_Y[i:min(i+batch_size*num_batch_per_gpu_transfer, train_reactants.batch_size)].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        j = i % batch_size*num_batch_per_gpu_transfer\n",
    "        outputs = critic(batch_reactants[j:j+batch_size], batch_products[j:j+batch_size], batch_rsigs[j:j+batch_size], batch_psigs[j:j+batch_size])\n",
    "        loss = criterion(outputs, batch_Y[j:j+batch_size].to(device))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Free memory\n",
    "    torch.cuda.empty_cache()    \n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "    # Get test loss\n",
    "    critic.eval()\n",
    "    temp_test_loss = []\n",
    "    for i in range(0, test_reactants.batch_size - batch_size, batch_size):\n",
    "        temp_test_loss.append(\n",
    "                        criterion(critic(test_reactants[i:i+batch_size].to(device), \n",
    "                                         test_products[i:i+batch_size].to(device), \n",
    "                                         test_rsigs[i:i+batch_size].to(device), \n",
    "                                         test_psigs[i:i+batch_size].to(device)) > 0.5, \n",
    "                                  test_Y[i:i+batch_size].to(device)).item()) \n",
    "        torch.cuda.empty_cache()    \n",
    "    test_loss.append(np.mean(temp_test_loss))\n",
    "    \n",
    "    # Free memory\n",
    "    torch.cuda.empty_cache()   \n",
    "\n",
    "    print ('Epoch {}, Loss: {:.4f}, Test Loss: {:.4f}'.format(epoch+1, loss_list[-1], test_loss[-1]))\n",
    "\n",
    "print(\"\\nFINAL TEST LOSS:\", test_loss[-1])\n",
    "\n",
    "plt.plot(loss_list[5:], label=\"training loss\")\n",
    "plt.plot(test_loss[5:], label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16390e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(critic, f\"models/supervised/critic/{steps}step.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87bc10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(critic(test_reactants, test_products, test_rsigs, test_psigs), train_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2caff4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e6dbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd269a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4234c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
