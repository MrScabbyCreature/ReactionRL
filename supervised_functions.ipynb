{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1cfa57a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MAIN_DIR\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f862867f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import itertools, functools\n",
    "from tabulate import tabulate\n",
    "\n",
    "from action_utils import *\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "660f7320",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_train_data(smile, steps):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "\n",
    "    df = pd.DataFrame(columns=['reactant', 'rsub', 'rcen', 'rsig', 'rsig_cs_indices', 'psub', 'pcen', 'psig', 'psig_cs_indices', 'product'])\n",
    "    index = []\n",
    "    \n",
    "    # Get sequences\n",
    "    try:\n",
    "        for i in range(steps):\n",
    "            actions = get_applicable_actions(mol)\n",
    "            if actions.shape[0] == 0:\n",
    "                break\n",
    "\n",
    "            # Apply a random action\n",
    "            rand_idx = np.random.randint(0, actions.shape[0])\n",
    "            product = apply_action(mol, *actions.iloc[rand_idx])\n",
    "\n",
    "            # Add it to df\n",
    "            df.loc[df.shape[0], :] = [Chem.MolToSmiles(mol)] + actions.iloc[rand_idx].tolist() + [Chem.MolToSmiles(product)]\n",
    "            index.append(actions.iloc[rand_idx].name)\n",
    "\n",
    "            # Next reactant = product\n",
    "            mol = product\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame(columns=['reactant', 'rsub', 'rcen', 'rsig', 'rsig_cs_indices', 'psub', 'pcen', 'psig', 'psig_cs_indices', 'product'])\n",
    "    \n",
    "    # Fix index\n",
    "    df.index = index\n",
    "    \n",
    "    # Make combinations for multi-step possibilities of source-->target\n",
    "    for i in range(df.shape[0]-1, 0, -1):\n",
    "        new_df = df.iloc[:i].copy()\n",
    "        new_df[\"product\"] = df.iloc[i][\"product\"]\n",
    "        df = pd.concat([df, new_df])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d58b5a",
   "metadata": {},
   "source": [
    "# Neural Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e1a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48289f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_hidden=1, hidden_size=50):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(num_hidden):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            self.hidden_layers.append(nn.ReLU())\n",
    "            \n",
    "        self.last_layer = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        for layer in self.hidden_layers:\n",
    "            out = layer(out)\n",
    "        out = self.last_layer(out)\n",
    "        return out\n",
    "\n",
    "def train(X, Y, num_hidden=1, hidden_size=50, lr=1e-2, bs=64, epochs=100):\n",
    "    train_X = torch.Tensor(X[:int(X.shape[0]*0.7)]).to(device)\n",
    "    train_Y = torch.Tensor(Y[:int(Y.shape[0]*0.7)]).to(device)\n",
    "\n",
    "    test_X = torch.Tensor(X[int(X.shape[0]*0.7):]).to(device)\n",
    "    test_Y = torch.Tensor(Y[int(Y.shape[0]*0.7):]).to(device)\n",
    "    \n",
    "    model = NeuralNet(train_X.shape[1], train_Y.shape[1], num_hidden=num_hidden, hidden_size=hidden_size).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  \n",
    "\n",
    "    loss_list = []\n",
    "    test_loss = []\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, train_X.shape[0], batch_size):\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(train_X[i:i+batch_size])\n",
    "            loss = criterion(outputs, train_Y[i:i+batch_size])\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.item())\n",
    "        print ('Epoch {}, Loss: {:.4f}'.format(epoch+1, loss.item()))\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss.append(criterion(model(test_X), test_Y).item()) \n",
    "    print(\"\\nFINAL TEST LOSS:\", test_loss[-1])\n",
    "        \n",
    "    plt.plot(loss_list[5:], label=\"training loss\")\n",
    "    plt.plot(test_loss[5:], label=\"test loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662cf55",
   "metadata": {},
   "source": [
    "# Helper functions and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835d1385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchdrug import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00cda6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def molecule_from_smile(smile):\n",
    "    try:\n",
    "        mol = data.Molecule.from_smiles(smile, atom_feature=\"pretrain\", bond_feature=\"pretrain\")\n",
    "    except Exception as e:\n",
    "        mol = data.Molecule.from_smiles(smile, atom_feature=\"pretrain\", bond_feature=\"pretrain\", with_hydrogen=True)\n",
    "    return mol\n",
    "\n",
    "def get_mol_embedding(model, smiles):\n",
    "    # deepchem - attribute masking\n",
    "    if isinstance(smiles, str):\n",
    "        mol = molecule_from_smile(smiles)\n",
    "    elif isinstance(smiles, list) or isinstance(smiles, pd.Series):\n",
    "        mol = list(map(molecule_from_smile, smiles))\n",
    "        mol = data.Molecule.pack(mol)\n",
    "    else:\n",
    "        mol = smiles\n",
    "    mol = mol.to(device)\n",
    "    emb = model(mol, mol.node_feature.float())[\"graph_feature\"]\n",
    "    return emb.detach()\n",
    "\n",
    "def get_atom_embedding(model, smiles, idx):\n",
    "    try:\n",
    "        mol = data.Molecule.from_smiles(smiles, atom_feature=\"pretrain\", bond_feature=\"pretrain\")\n",
    "        emb = model(mol, mol.node_feature.float())[\"node_feature\"][idx]\n",
    "    except Exception as e:\n",
    "        mol = data.Molecule.from_smiles(smiles, atom_feature=\"pretrain\", bond_feature=\"pretrain\", with_hydrogen=True)\n",
    "        emb = model(mol, mol.node_feature.float())[\"node_feature\"][idx]\n",
    "    return emb.detach()\n",
    "\n",
    "def get_action_embedding(model, action_df):\n",
    "    rsub, rcen, rsig, _, psub, pcen, psig, __ = [action_df[c] for c in action_df.columns]\n",
    "#     print(get_mol_embedding(model, rsub).shape)\n",
    "#     print(get_atom_embedding(model, rsig, rcen).shape)\n",
    "#     print(get_mol_embedding(model, rsig).shape)\n",
    "#     print(get_mol_embedding(model, psub).shape)\n",
    "#     print(get_atom_embedding(model, psig, pcen).shape)\n",
    "#     print(get_mol_embedding(model, psig).shape)\n",
    "    embedding = torch.concatenate([\n",
    "#                         get_mol_embedding(model, rsub), \n",
    "#                         get_atom_embedding(model, rsig, rcen) / 5, \n",
    "                        get_mol_embedding(model, rsig), \n",
    "#                         get_mol_embedding(model, psub), \n",
    "#                         get_atom_embedding(model, psig, pcen) / 5, \n",
    "                        get_mol_embedding(model, psig)\n",
    "                    ], axis=1)\n",
    "    return embedding\n",
    "\n",
    "def get_action_embedding_from_packed_molecule(model, rsig, psig):\n",
    "    embedding = torch.concatenate([\n",
    "                            get_mol_embedding(model, rsig), \n",
    "                           get_mol_embedding(model, psig)\n",
    "                    ], axis=1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddae391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_dataset_embeddings(model):\n",
    "    batch_size = 2048\n",
    "    action_embeddings = []\n",
    "    for i in tqdm.tqdm(range(0, action_dataset.shape[0], batch_size)):\n",
    "        batch_rsig = action_rsigs[i:min(i+batch_size, action_dataset.shape[0])].to(device)\n",
    "        batch_psig = action_psigs[i:min(i+batch_size, action_dataset.shape[0])].to(device)\n",
    "        action_embeddings.append(get_action_embedding_from_packed_molecule(model, batch_rsig, batch_psig))\n",
    "#         del batch_rsig, batch_psig\n",
    "    action_embeddings = torch.concatenate(action_embeddings)\n",
    "    return action_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92e20cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_emb_indices_and_correct_idx(row):\n",
    "    if isinstance(row, tuple): # For pandas iterrows\n",
    "        row = row[1]\n",
    "    \n",
    "    # Applicable indices\n",
    "    applicable_actions_df = get_applicable_actions(Chem.MolFromSmiles(row[\"reactant\"]))\n",
    "    if applicable_actions_df.shape[0] == 0:\n",
    "        # If there are no applicable actions detected (rdkit problems)\n",
    "        indices_used_for_data = np.where((action_dataset.index == row.name))[0]\n",
    "        correct_idx = 0\n",
    "    else:\n",
    "        indices_used_for_data = np.where(action_dataset.index.isin(applicable_actions_df.index))[0]\n",
    "        \n",
    "        # Correct index\n",
    "        applicable_actions_df = applicable_actions_df.loc[action_dataset.iloc[indices_used_for_data].index]\n",
    "        correct_applicable_idx = (applicable_actions_df.index == row.name).argmax()\n",
    "        correct_action_idx = indices_used_for_data[correct_applicable_idx]\n",
    "    \n",
    "    return indices_used_for_data, correct_applicable_idx, correct_action_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a56268a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking(pred, emb_for_comparison, correct_index, distance=\"euclidean\", k=None):\n",
    "    '''\n",
    "    Get the rank of the prediction from the applicable actions.\n",
    "    Returns (rank, [list_of_indices before <rank>])\n",
    "    '''\n",
    "    if distance == \"euclidean\":\n",
    "        dist = ((emb_for_comparison-pred)**2).sum(axis=1)\n",
    "    elif distance == \"cosine\":\n",
    "        dist = 1 - torch.mm(emb_for_comparison, pred.view(-1, 1)).view(-1)/(torch.linalg.norm(emb_for_comparison, axis=1)*torch.linalg.norm(pred))\n",
    "\n",
    "    # Get rank\n",
    "    sorted_idx = dist.argsort()\n",
    "    rank = (dist[sorted_idx] == dist[correct_index]).nonzero()[0]\n",
    "    list_of_indices = dist[sorted_idx[:rank]]\n",
    "    \n",
    "    # When the rank(correct_index) < k, then returns <rank, list>. So this extra condition - add some indices after rank(correct_index) to the list\n",
    "    if k is not None:\n",
    "        return sorted_idx[:k]\n",
    "    return rank, list_of_indices\n",
    "\n",
    "def get_top_k_indices(pred, emb_for_comparison, correct_index, distance=\"euclidean\", k=1):\n",
    "    return get_ranking(pred, emb_for_comparison, correct_index, distance, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7213fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/mangye16/ReID-Survey\n",
    "def euclidean_dist(x, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x: pytorch Variable, with shape [m, d]\n",
    "      y: pytorch Variable, with shape [n, d]\n",
    "    Returns:\n",
    "      dist: pytorch Variable, with shape [m, n]\n",
    "    \"\"\"\n",
    "    m, n = x.size(0), y.size(0)\n",
    "    xx = torch.pow(x, 2).sum(1, keepdim=True).expand(m, n)\n",
    "    yy = torch.pow(y, 2).sum(1, keepdim=True).expand(n, m).t()\n",
    "    dist = xx + yy\n",
    "    dist.addmm_(1, -2, x, y.t())\n",
    "    dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
    "    return dist\n",
    "\n",
    "def cosine_dist(x, y):\n",
    "    xy = x.matmul(y.t())\n",
    "\n",
    "    m, n = x.size(0), y.size(0)\n",
    "    xx = torch.linalg.norm(x, axis=1).expand(n, m).t()\n",
    "    yy = torch.linalg.norm(y, axis=1).expand(m, n)\n",
    "    \n",
    "    return 1 - xy / (xx*yy)\n",
    "\n",
    "\n",
    "def softmax_weights(dist, mask):\n",
    "    max_v = torch.max(dist * mask, dim=1, keepdim=True)[0]\n",
    "    diff = dist - max_v\n",
    "    Z = torch.sum(torch.exp(diff) * mask, dim=1, keepdim=True) + 1e-6 # avoid division by zero\n",
    "    W = torch.exp(diff) * mask / Z\n",
    "    return W\n",
    "\n",
    "class WeightedRegularizedTriplet(object):\n",
    "    def __init__(self, dist=\"euclidean\"):\n",
    "        self.ranking_loss = nn.SoftMarginLoss()\n",
    "        self.dist = dist\n",
    "\n",
    "    def __call__(self, global_feat, labels):\n",
    "        if self.dist==\"euclidean\":\n",
    "            dist_mat = euclidean_dist(global_feat, global_feat)\n",
    "        elif self.dist==\"cosine\":\n",
    "            dist_mat = cosine_dist(global_feat, global_feat) ####### NEEEDS TO BE CHANGED!!!!!!!!!!!\n",
    "\n",
    "        N = dist_mat.size(0)\n",
    "        # shape [N, N]\n",
    "        is_pos = labels.expand(N, N).eq(labels.expand(N, N).t()).float()\n",
    "        is_neg = labels.expand(N, N).ne(labels.expand(N, N).t()).float()\n",
    "\n",
    "        # `dist_ap` means distance(anchor, positive)\n",
    "        # both `dist_ap` and `relative_p_inds` with shape [N, 1]\n",
    "        dist_ap = dist_mat * is_pos\n",
    "        dist_an = dist_mat * is_neg\n",
    "\n",
    "        weights_ap = softmax_weights(dist_ap, is_pos)\n",
    "        weights_an = softmax_weights(-dist_an, is_neg)\n",
    "        furthest_positive = torch.sum(dist_ap * weights_ap, dim=1)\n",
    "        closest_negative = torch.sum(dist_an * weights_an, dim=1)\n",
    "\n",
    "        y = furthest_positive.new().resize_as_(furthest_positive).fill_(1)\n",
    "        loss = self.ranking_loss(closest_negative - furthest_positive, y)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2011ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.GIN = torch.load(\"models/zinc2m_gin.pth\")\n",
    "        self.DENSE = torch.load(\"datasets/my_uspto/supervised_zinc_gin/mse_model.pth\")\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.GIN(x1, x1.node_feature.float())[\"graph_feature\"]\n",
    "        out2 = self.GIN(x2, x2.node_feature.float())[\"graph_feature\"]\n",
    "        \n",
    "        out = torch.concatenate([out1, out2], axis=1)\n",
    "        out = self.DENSE(out)\n",
    "        return out\n",
    "    \n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.GIN = torch.load(\"models/zinc2m_gin.pth\")\n",
    "        self.DENSE = NeuralNet(self.GIN.output_dim*4, 1, num_hidden=num_hidden, hidden_size=hidden_size).to(device)\n",
    "    \n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        out1 = self.GIN(x1, x1.node_feature.float())[\"graph_feature\"]\n",
    "        out2 = self.GIN(x2, x2.node_feature.float())[\"graph_feature\"]\n",
    "        out3 = self.GIN(x3, x3.node_feature.float())[\"graph_feature\"]\n",
    "        out4 = self.GIN(x4, x4.node_feature.float())[\"graph_feature\"]\n",
    "        \n",
    "        out = torch.concatenate([out1, out2, out3, out4], axis=1)\n",
    "        out = self.DENSE(out)\n",
    "        return out\n",
    "    \n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.GIN = torch.load(\"models/zinc2m_gin.pth\")\n",
    "        self.actor = NeuralNet(self.GIN.output_dim*2, self.GIN.output_dim*2, num_hidden=3, hidden_size=500)\n",
    "        self.critic = NeuralNet(self.GIN.output_dim*4, 1, num_hidden=2, hidden_size=256)\n",
    "    \n",
    "    def forward(self, reac, prod, rsig, psig, out_type=\"both\"):\n",
    "        '''\n",
    "        If out_type=\"actor\", returns actions\n",
    "        If out_type=\"critic\", returns q_value\n",
    "        If out_type=\"both\", returns [actions, q_value]\n",
    "        '''\n",
    "        reac_out = self.GIN(reac, reac.node_feature.float())[\"graph_feature\"]\n",
    "        prod_out = self.GIN(prod, prod.node_feature.float())[\"graph_feature\"]\n",
    "    \n",
    "        output = []\n",
    "        if out_type in [\"both\", \"actor\"]:\n",
    "            output.append(self.actor(torch.concatenate([reac_out, prod_out], axis=1)))\n",
    "\n",
    "        if out_type in [\"both\", \"critic\"]:\n",
    "            psig_out = self.GIN(psig, psig.node_feature.float())[\"graph_feature\"]\n",
    "            rsig_out = self.GIN(rsig, rsig.node_feature.float())[\"graph_feature\"]\n",
    "            output.append(self.critic(torch.concatenate([reac_out, prod_out, rsig_out, psig_out], axis=1)))\n",
    "        \n",
    "        if len(output) == 1:\n",
    "            return output[0]\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
