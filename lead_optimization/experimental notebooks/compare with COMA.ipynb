{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e86d5b6c-6e4e-48c2-9bb6-a6a500c9ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.environ[\"MAIN_DIR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2808a56f-5ff7-4b54-8682-f31cb7234ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from IPython.display import display\n",
    "import tqdm\n",
    "from rewards.properties import logP, qed, drd2\n",
    "\n",
    "mfs = Chem.MolFromSmiles\n",
    "mts = Chem.MolToSmiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62721b93-cd54-4a16-bb8e-6bf88e294af6",
   "metadata": {},
   "source": [
    "### Get start molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67eaf6bf-251f-4316-86c3-d6d0c16f4057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.6 s, sys: 2.11 s, total: 29.7 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "# All start molecules\n",
    "%time start_mols = list(Chem.SDMolSupplier('datasets/offlineRL/Enamine_Building_Blocks_Stock_266483cmpd_20230901.sdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d6b68-bb00-496b-90f4-33cfe51d7234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████████▏                                           | 58001/266483 [01:05<04:20, 801.00it/s]"
     ]
    }
   ],
   "source": [
    "from action_utils import get_applicable_actions\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rewards.properties import similarity\n",
    "\n",
    "temp_mols = []\n",
    "\n",
    "def error_handled_get_applicable_actions(mol):\n",
    "    try:\n",
    "        return get_applicable_actions(mol)\n",
    "    except:\n",
    "        return np.zeros(shape=(0,0))\n",
    "\n",
    "with Pool(14) as P:\n",
    "    for mol, df in tqdm.tqdm(zip(start_mols, P.imap(error_handled_get_applicable_actions, start_mols, chunksize=100)), total=len(start_mols)):\n",
    "        if df.shape[0] > 0:\n",
    "            temp_mols.append(mol)\n",
    "\n",
    "temp_smiles = list(map(mts, temp_mols))\n",
    "\n",
    "start_mols_df = pd.DataFrame(temp_smiles, columns=[\"SMILES\"])\n",
    "start_mols_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa4b7c-56f9-495d-bd95-46aa309087f0",
   "metadata": {},
   "source": [
    "### Load dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef2d27-af75-46f9-9878-4441520dc69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROPERTY_NAME = \"drd2\"\n",
    "# SCORING_FT = drd2\n",
    "\n",
    "# PROPERTY_NAME = \"qed\"\n",
    "# SCORING_FT = qed\n",
    "\n",
    "# PROPERTY_NAME = \"logp04\"\n",
    "# SCORING_FT = logP\n",
    "\n",
    "PROPERTY_NAME = \"logp06\"\n",
    "SCORING_FT = logP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd3bde-9933-4d15-a893-12db19b0db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file\n",
    "input_data_dir = os.path.join(\"datasets/coma\", PROPERTY_NAME)\n",
    "filepath_test = os.path.join(input_data_dir, \"rdkit_test.txt\")\n",
    "print(filepath_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f1fab-96a2-4abd-862f-4bfb9050e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_dir = f\"results/eval_on_coma_{PROPERTY_NAME}\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "filepath_output = os.path.join(output_dir, f\"{PROPERTY_NAME}.csv\")\n",
    "print(filepath_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff03340-f704-4991-bede-fb36bf477eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "test_data = pd.read_csv(filepath_test, header=None)\n",
    "test_data.columns = [\"smiles\"]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c14a67-d7ad-4988-824a-e9e9b01ba9dc",
   "metadata": {},
   "source": [
    "### Functions to get some start molecules and for generating molecules with path length 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d033b-bc56-4e65-8802-231cf8c2e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem.Scaffolds.MurckoScaffold import GetScaffoldForMol\n",
    "import tqdm\n",
    "from rdkit.Chem import rdFMCS\n",
    "from functools import partial\n",
    "\n",
    "def my_conv_sim_fn(mol, sm):\n",
    "    return similarity(mfs(sm), mol)\n",
    "\n",
    "def my_conv_scaf_sim_fn(scaf, sm):\n",
    "    return similarity(GetScaffoldForMol(mfs(sm)), scaf)\n",
    "\n",
    "def my_conv_mcs_fn(mol, sm):\n",
    "    return rdFMCS.FindMCS([mol, GetScaffoldForMol(mfs(sm))]).numAtoms\n",
    "\n",
    "\n",
    "def get_start_mols(smiles, n):\n",
    "    mol = mfs(smiles)\n",
    "    start_mol_list = []\n",
    "\n",
    "    \n",
    "    # Similarity\n",
    "    mol_sim_df = pd.Series(Pool(14).map(partial(my_conv_sim_fn, mol), start_mols_df[\"SMILES\"], chunksize=100))\n",
    "    start_mol_list.extend(start_mols_df.loc[mol_sim_df.sort_values(ascending=False).index[:3*n]][\"SMILES\"].values.tolist())\n",
    "    \n",
    "    # Scaffold by similarity\n",
    "    scaf = GetScaffoldForMol(mol)\n",
    "    scaf_sim_df = pd.Series(Pool(14).map(partial(my_conv_scaf_sim_fn, scaf), start_mols_df[\"SMILES\"], chunksize=100))\n",
    "    start_mol_list.extend(start_mols_df.loc[scaf_sim_df.sort_values(ascending=False).index[:3*n]][\"SMILES\"].values.tolist())\n",
    "\n",
    "    # MCS\n",
    "    scaf_mcs_df = pd.Series(Pool(14).map(partial(my_conv_mcs_fn, mol), start_mols_df[\"SMILES\"], chunksize=100))\n",
    "    val = scaf_mcs_df.sort_values(ascending=False).values[n-1]\n",
    "    start_mol_list.extend(start_mols_df.loc[scaf_sim_df[scaf_mcs_df >= val].sort_values(ascending=False).index[:3*n]][\"SMILES\"].values.tolist())\n",
    "\n",
    "    start_mol_list = np.random.choice(np.unique(start_mol_list), size=(3*n,), replace=False)\n",
    "    return start_mol_list\n",
    "\n",
    "# %time get_start_mols(test_data[\"smiles\"][0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce15f9-aef6-4439-9d55-5973d0fb7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9bb0eb-95a8-460f-90f2-598cde56f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run lead_optimization/experimental\\ notebooks/supervised_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca09c4c-e490-4194-bb4b-9eac9178a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dataset = pd.read_csv(\"datasets/my_uspto/action_dataset-filtered.csv\", index_col=0)\n",
    "action_dataset = action_dataset.loc[action_dataset[\"action_tested\"] & action_dataset[\"action_works\"]]\n",
    "action_dataset = action_dataset[[\"rsub\", \"rcen\", \"rsig\", \"rsig_cs_indices\", \"psub\", \"pcen\", \"psig\", \"psig_cs_indices\"]]\n",
    "print(action_dataset.shape)\n",
    "\n",
    "action_rsigs = data.Molecule.pack(list(map(molecule_from_smile, action_dataset[\"rsig\"])))\n",
    "action_psigs = data.Molecule.pack(list(map(molecule_from_smile, action_dataset[\"psig\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d30936-3808-4265-8464-44ced62d96dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def get_topk_predictions(model, source_list, target_list, topk=10):\n",
    "    # Convert to mols\n",
    "    if isinstance(source_list, pd.Series):\n",
    "        source_list = source_list.tolist()\n",
    "    if isinstance(target_list, pd.Series):\n",
    "        target_list = target_list.tolist()\n",
    "    tt = time.time()\n",
    "    sources = data.Molecule.pack(list(map(molecule_from_smile, source_list)))\n",
    "    targets = data.Molecule.pack(list(map(molecule_from_smile, target_list)))\n",
    "    print(f\"Took {time.time() - tt}s to pack molecules.\")\n",
    "\n",
    "    # Predictions\n",
    "    batch_size = 1024\n",
    "#     pred = model(sources, targets, None, None, \"actor\").detach()\n",
    "    pred = torch.concatenate([model(sources[i:min(i+batch_size, sources.batch_size)].to(device), \n",
    "                                 targets[i:min(i+batch_size, sources.batch_size)].to(device), None, None, \"actor\").detach() for i in range(0, sources.batch_size, batch_size)], axis=0)\n",
    "\n",
    "    action_embeddings = get_action_dataset_embeddings(model.GIN)\n",
    "\n",
    "    # Get applicable actions for source(s)\n",
    "    applicable_action_indices_list = []\n",
    "    \n",
    "    with Pool(30) as p:\n",
    "        for idxes in tqdm.tqdm(p.imap(functools.partial(get_emb_indices_and_correct_idx, no_correct_idx=True), \n",
    "                                      [{\"reactant\": source_list[i]} for i in range(pred.shape[0])], chunksize=10),\n",
    "                              total=pred.shape[0]):\n",
    "            applicable_action_indices_list.append(idxes)\n",
    "\n",
    "    # Sort by critic's Q\n",
    "    dict_of_list_of_indices = {}\n",
    "    \n",
    "    for i in tqdm.tqdm(range(pred.shape[0])):\n",
    "        pred_for_i = pred[i]\n",
    "        adi = applicable_action_indices_list[i]\n",
    "        if len(adi) == 0:\n",
    "            dict_of_list_of_indices[i] = np.array([])\n",
    "            continue\n",
    "\n",
    "        # Get top 50 for actor\n",
    "        dist = torch.linalg.norm(action_embeddings[adi] - pred[i], axis=1)\n",
    "        dict_of_list_of_indices[i] = adi[torch.argsort(dist)[:50].cpu().numpy().astype(np.int64)]\n",
    "\n",
    "    # Sort with critic's Q\n",
    "    i_sorted = list(range(pred.shape[0]))\n",
    "    action_indices = np.concatenate([dict_of_list_of_indices[i] for i in i_sorted])\n",
    "    state_indices = np.concatenate([np.full_like(dict_of_list_of_indices[i], i) for i in i_sorted])\n",
    "    critic_qs = []\n",
    "    for i in tqdm.tqdm(range(0, action_indices.shape[0], batch_size)):\n",
    "        batch_reactants = sources[state_indices[i:i+batch_size]]\n",
    "        batch_products = targets[state_indices[i:i+batch_size]]\n",
    "        batch_rsigs = action_rsigs[action_indices[i:i+batch_size]]\n",
    "        batch_psigs = action_psigs[action_indices[i:i+batch_size]]\n",
    "        critic_qs.append(ac(batch_reactants.to(device), batch_products.to(device), batch_rsigs.to(device), batch_psigs.to(device), \"critic\").detach().cpu().numpy())\n",
    "\n",
    "    critic_qs = np.concatenate(critic_qs)\n",
    "\n",
    "    # Get action predictions\n",
    "    action_pred_indices = []\n",
    "    start = 0\n",
    "    for i in tqdm.tqdm(i_sorted):\n",
    "        end = start + dict_of_list_of_indices[i].shape[0]\n",
    "        i_critic_qs = critic_qs[start:end]\n",
    "\n",
    "        action_pred_indices.append(dict_of_list_of_indices[i][i_critic_qs.reshape(-1).argsort()[::-1]][:topk])\n",
    "        start = end\n",
    "\n",
    "    return action_pred_indices\n",
    "    \n",
    "# %time pred = get_topk_predictions(ac, source_list[:100], target_list[target_list_idx][:100])\n",
    "# print(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1668c-4fe5-4223-812d-b909af242e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_actions_on_reactant(args):\n",
    "    reactant, action_dataset_idx = args\n",
    "    listy = []\n",
    "    for idx in action_dataset_idx:\n",
    "        try:\n",
    "            listy.append(Chem.MolToSmiles(apply_action(Chem.MolFromSmiles(reactant), *action_dataset.iloc[idx])))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return listy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133e026-af30-4ffc-94e0-3c240efa582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_from_smiles(s1, s2):\n",
    "    return similarity(Chem.MolFromSmiles(s1), Chem.MolFromSmiles(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0af61-5a72-4e27-ac8f-81c615013ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predictions(source_list, target_list, steps=5, topk=5, limit=1000):    \n",
    "    target_list_idx = np.arange(target_list.shape[0])\n",
    "    sim_dict = {}\n",
    "    trajectory_dict = {str(i): source_list[i] for i in range(len(source_list))} # Keeps track of trajectory in dict format (need hash keeys for quick access)\n",
    "    source_keys = list(map(str, np.arange(len(source_list)))) # Map for index to keys of previous step in trajectory\n",
    "    \n",
    "    # RUN -----------------------\n",
    "    for i_step in range(1, steps+1): \n",
    "        print(\"Running prediction for step\", i_step)\n",
    "        # Get action predictions\n",
    "        pred = get_topk_predictions(ac, source_list, target_list[target_list_idx], topk=topk)\n",
    "    \n",
    "        # get products\n",
    "        temp_source_keys = []\n",
    "        temp_source_list = []\n",
    "        with Pool(14) as p:\n",
    "            print(\"Applying actions for step\", i_step)\n",
    "            for i, product_list in tqdm.tqdm(enumerate(p.imap(apply_actions_on_reactant, zip(source_list, pred), chunksize=10)), total=len(pred)):\n",
    "                for _i, product in enumerate(product_list):\n",
    "                    key = f\"{source_keys[i]}_{_i}\"\n",
    "                    trajectory_dict[key] = product\n",
    "                    sim_dict[key] = get_similarity_from_smiles(product, target_list[int(key.split(\"_\")[0])])\n",
    "                    temp_source_keys.append(key)\n",
    "                    temp_source_list.append(product)\n",
    "    \n",
    "        print(\"Getting top some sim products for each s-t pair\")\n",
    "        temp_source_keys = np.array(temp_source_keys)\n",
    "        temp_source_list = np.array(temp_source_list)\n",
    "        temp_source_sim = np.vectorize(sim_dict.get)(temp_source_keys)\n",
    "        temp_source_argsort = np.argsort(temp_source_sim)\n",
    "        temp_source_st_idx = np.array(list(map(lambda x: int(x.split(\"_\")[0]), temp_source_keys)))\n",
    "        temp_source_indices = []\n",
    "        for t_i in range(target_list.shape[0]):\n",
    "            temp_source_indices.append((temp_source_argsort[temp_source_st_idx == t_i])[:limit])\n",
    "    \n",
    "        temp_source_indices = np.concatenate(temp_source_indices)\n",
    "    \n",
    "        # update source list and source_keys for next step\n",
    "        source_keys = temp_source_keys[temp_source_indices]\n",
    "        source_list = temp_source_list[temp_source_indices]\n",
    "        target_list_idx = list(map(lambda x: int(x.split(\"_\")[0]), source_keys))\n",
    "\n",
    "    return trajectory_dict, sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbfee77-b37b-4025-896c-3c88b6c69ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "import glob\n",
    "file_string = f\"models/supervised/actor-critic/steps=5||actor_loss=PG||negative_selection=random/model.pth\"\n",
    "ac = torch.load(glob.glob(file_string)[0], map_location=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa4badf-0ab6-4d52-84d9-de2f73b6e304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t_i, test_smile in enumerate(test_data[\"smiles\"]): \n",
    "    %time source_list = np.array(get_start_mols(test_smile, 10))\n",
    "    target_list = np.array([test_smile for i in range(len(source_list))])\n",
    "\n",
    "    print(\"$$$$$$$$$$$$$$$$\")\n",
    "    print(f\"$ Test mol: {t_i} $\")\n",
    "    print(\"$$$$$$$$$$$$$$$$\")\n",
    "\n",
    "    traj_d, sim_d = run_predictions(source_list, target_list, steps=5)\n",
    "\n",
    "    pickle.dump({\"traj\": traj_d, \"sim\":sim_d}, open(f\"results/eval_on_coma_{PROPERTY_NAME}/{t_i}.pickle\", 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
